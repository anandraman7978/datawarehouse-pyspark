Summary
Project has been designed based on datawarehousing concepts in Postgres and ETL pipeline using Python. To complete the project, have defined fact and dimension tables for a star schema , and have written an ETL pipeline that will load the files from S3 to staging tables through copy commands and then load the data from staging to main fact and dimension tables. It also involves the creation of cluster of in aws redshift and loading it into redshift postgres.

How to run:


Step 1: Run create_tables.py to create your database and tables.
Step 2: Run etl.py to run the etl piplines




Data and Design:

Files Involved:
Song Dataset:The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song

logs Dataset: log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.




Tables Involved:
Fact Table
songplays - records in log data associated with song plays i.e. records with page NextSong 

Dimension Tables
users - users in the app
songs - songs in music database
artists - artists in music database
time - timestamps of records in songplays broken down into specific units

Scripts:
create_tables.py drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
etl.py reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
sql_queries.py contains all your sql queries, and is imported into the last three files above.
README.md provides discussion on your project.