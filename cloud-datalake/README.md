Summary
Used Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. 
1. load data from S3, 
2. process the data into analytics tables using Spark
3. load them back into S3 using AWS cluster.

How to run:


Step 1: Run etl.py to run the etl piplines




Data and Design:

Files Involved:
Song Dataset:The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song

logs Dataset: log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.




Tables Involved:
Fact Table
songplays - records in log data associated with song plays i.e. records with page NextSong 

Dimension Tables
users - users in the app
songs - songs in music database
artists - artists in music database
time - timestamps of records in songplays broken down into specific units

Scripts:
etl.py reads and processes files from song_data and log_data and loads them into your tables.
1. load data from S3 
2. process the data into analytics tables using Spark
3. load them back into S3 using AWS cluster.
README.md provides discussion on your project.